{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b580d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data in python\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "498e2af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data-type of the array's elements.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "None\n",
      "\n",
      "Returns\n",
      "-------\n",
      "d : numpy dtype object\n",
      "\n",
      "See Also\n",
      "--------\n",
      "numpy.dtype\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> x\n",
      "array([[0, 1],\n",
      "       [2, 3]])\n",
      ">>> x.dtype\n",
      "dtype('int32')\n",
      ">>> type(x.dtype)\n",
      "<type 'numpy.dtype'>\n",
      "Help on function read_csv in module pandas.io.parsers.readers:\n",
      "\n",
      "read_csv(filepath_or_buffer: 'FilePath | ReadCsvBuffer[bytes] | ReadCsvBuffer[str]', sep=<no_default>, delimiter=None, header='infer', names=<no_default>, index_col=None, usecols=None, squeeze=None, prefix=<no_default>, mangle_dupe_cols=True, dtype: 'DtypeArg | None' = None, engine: 'CSVEngine | None' = None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=None, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression: 'CompressionOptions' = 'infer', thousands=None, decimal: 'str' = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, encoding_errors: 'str | None' = 'strict', dialect=None, error_bad_lines=None, warn_bad_lines=None, on_bad_lines=None, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None, storage_options: 'StorageOptions' = None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handle (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, None, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, optional, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). If ``names`` are given, the document\n",
      "        header row(s) are not taken into account. For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    \n",
      "        .. deprecated:: 1.4.0\n",
      "            Append ``.squeeze(\"columns\")`` to the call to ``read_csv`` to squeeze\n",
      "            the data.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    \n",
      "        .. deprecated:: 1.4.0\n",
      "           Use a list comprehension on the DataFrame's columns after calling ``read_csv``.\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python', 'pyarrow'}, optional\n",
      "        Parser engine to use. The C and pyarrow engines are faster, while the python engine\n",
      "        is currently more feature-complete. Multithreading is currently only supported by\n",
      "        the pyarrow engine.\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            The \"pyarrow\" engine was added as an *experimental* engine, and some features\n",
      "            are unsupported, or may not work correctly, with this engine.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparsable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           ``TextFileReader`` is a context manager.\n",
      "    compression : str or dict, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and '%s' is\n",
      "        path-like, then detect compression from the following extensions: '.gz',\n",
      "        '.bz2', '.zip', '.xz', or '.zst' (otherwise no compression). If using\n",
      "        'zip', the ZIP file must contain only one data file to be read in. Set to\n",
      "        ``None`` for no decompression. Can also be a dict with key ``'method'`` set\n",
      "        to one of {``'zip'``, ``'gzip'``, ``'bz2'``, ``'zstd'``} and other\n",
      "        key-value pairs are forwarded to ``zipfile.ZipFile``, ``gzip.GzipFile``,\n",
      "        ``bz2.BZ2File``, or ``zstandard.ZstdDecompressor``, respectively. As an\n",
      "        example, the following could be passed for Zstandard decompression using a\n",
      "        custom compression dictionary:\n",
      "        ``compression={'method': 'zstd', 'dict_data': my_compression_dict}``.\n",
      "    \n",
      "        .. versionchanged:: 1.4.0 Zstandard support.\n",
      "    \n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "           When ``encoding`` is ``None``, ``errors=\"replace\"`` is passed to\n",
      "           ``open()``. Otherwise, ``errors=\"strict\"`` is passed to ``open()``.\n",
      "           This behavior was previously only the case for ``engine=\"python\"``.\n",
      "    \n",
      "        .. versionchanged:: 1.3.0\n",
      "    \n",
      "           ``encoding_errors`` is a new argument. ``encoding`` has no longer an\n",
      "           influence on how encoding errors are handled.\n",
      "    \n",
      "    encoding_errors : str, optional, default \"strict\"\n",
      "        How encoding errors are treated. `List of possible values\n",
      "        <https://docs.python.org/3/library/codecs.html#error-handlers>`_ .\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, optional, default ``None``\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will be dropped from the DataFrame that is\n",
      "        returned.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    warn_bad_lines : bool, optional, default ``None``\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    \n",
      "        .. deprecated:: 1.3.0\n",
      "           The ``on_bad_lines`` parameter should be used instead to specify behavior upon\n",
      "           encountering a bad line instead.\n",
      "    on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'\n",
      "        Specifies what to do upon encountering a bad line (a line with too many fields).\n",
      "        Allowed values are :\n",
      "    \n",
      "            - 'error', raise an Exception when a bad line is encountered.\n",
      "            - 'warn', raise a warning when a bad line is encountered and skip that line.\n",
      "            - 'skip', skip bad lines without raising or warning when they are encountered.\n",
      "    \n",
      "        .. versionadded:: 1.3.0\n",
      "    \n",
      "        .. versionadded:: 1.4.0\n",
      "    \n",
      "            - callable, function with signature\n",
      "              ``(bad_line: list[str]) -> list[str] | None`` that will process a single\n",
      "              bad line. ``bad_line`` is a list of strings split by the ``sep``.\n",
      "              If the function returns ``None``, the bad line will be ignored.\n",
      "              If the function returns a new list of strings with more elements than\n",
      "              expected, a ``ParserWarning`` will be emitted while dropping extra elements.\n",
      "              Only supported when ``engine=\"python\"``\n",
      "    \n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are ``None`` or 'high' for the ordinary converter,\n",
      "        'legacy' for the original lower precision pandas converter, and\n",
      "        'round_trip' for the round-trip converter.\n",
      "    \n",
      "        .. versionchanged:: 1.2\n",
      "    \n",
      "    storage_options : dict, optional\n",
      "        Extra options that make sense for a particular storage connection, e.g.\n",
      "        host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "        are forwarded to ``urllib`` as header options. For other URLs (e.g.\n",
      "        starting with \"s3://\", and \"gcs://\") the key-value pairs are forwarded to\n",
      "        ``fsspec``. Please see ``fsspec`` and ``urllib`` for more details.\n",
      "    \n",
      "        .. versionadded:: 1.2\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help \n",
    "np.info(np.ndarray.dtype) # help for numpy\n",
    "help(pd.read_csv)    #help for pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4917ac3",
   "metadata": {},
   "source": [
    "## Plain text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bf9f9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'huck_finn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "157b8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename, mode='w')  \n",
    "# w, x - Create a new file if it does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b8c9d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename, mode='r') # open file for reading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40bbe5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = file.read() #read files content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3b90dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(file.closed) # check whether file is closed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c9ede547",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close() # Close file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3b4cc031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d93d5",
   "metadata": {},
   "source": [
    "- \"r\" - Read - Default value. Opens a file for reading, **error** if the file does not exist\n",
    "\n",
    "- \"a\" - Append - Opens a file for appending, creates the file if it does not exist\n",
    "\n",
    "- \"w\" - Write - Opens a file for writing, creates the file if it does not exist\n",
    "\n",
    "- \"x\" - Create - Creates the specified file, returns an error if the file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9727a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24316623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write(\"I am good in Python now\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "66946191",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2c3ffdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open and read the file after writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2bbe0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(filename, mode=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26ac452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am good in Python now\n"
     ]
    }
   ],
   "source": [
    "print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ce64148",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(filename, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "57eeb505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write(\"Life is set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c9aa96c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write(\"I am having fun with coding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "afe7d4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc4b02bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.write('Beautigul file\\nin this soup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e6295ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e731f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life is setI am having fun with coding\n",
      "\n",
      "Beautigul file\n",
      "\n",
      "in this soup\n"
     ]
    }
   ],
   "source": [
    "with open('huck_finn.txt','r') as file:\n",
    "    print(file.readline()) #read a single line\n",
    "    print(file.readline())\n",
    "    print(file.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526f6c8f",
   "metadata": {},
   "source": [
    "# Pickled Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ddeb4",
   "metadata": {},
   "source": [
    "In Python, we work with high-level data structures such as lists, tuples, and sets. However, when we want to store these objects in memory, they need to be converted into a sequence of bytes that the computer can understand. This process is called **serialization**.\n",
    "\n",
    "The next time we want to access the same data structure, this sequence of bytes must be converted back into the high-level object in a process known as **deserialization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01b166cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a2b97101",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename1='pickled_fruit.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e66b9ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'pickled_fruit.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11648\\821951558.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#create pickle file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'pickled_fruit.pkl'"
     ]
    }
   ],
   "source": [
    "file = open(filename1, mode='x') #create pickle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6b788327",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11648\\3086206189.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pickled_fruit.pkl'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpickled_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#EOFError: Ran out of input-pickle file is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "with open('pickled_fruit.pkl','rb') as file:\n",
    "    pickled_data=pickle.load(file) \n",
    "    \n",
    "    \n",
    "#EOFError: Ran out of input-pickle file is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0d81b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits=['strawberry','Pineapple','Rasberry','Mixed_Fruit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e7f6a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a text file, write the list to it using the dumps() function\n",
    "with open('pickled_fruit.pkl','wb') as file: #open a text file\n",
    "    pickle.dump(fruits, file)    #serialize the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bddeb8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "476c99b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strawberry', 'Pineapple', 'Rasberry', 'Mixed_Fruit']\n"
     ]
    }
   ],
   "source": [
    "#deserialize file and print the list:\n",
    "\n",
    "with open('pickled_fruit.pkl','rb') as file:\n",
    "    \n",
    "    fruits_loaded=pickle.load(file)  #deserialize using load()\n",
    "    print(fruits_loaded) #print the fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a9776134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fruits_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e907f1d",
   "metadata": {},
   "source": [
    "# Serializing Machine Learning Models with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4e059421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# generate regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=0.1, random_state=1)\n",
    "\n",
    "# train regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a66a8959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model intercept : -0.010109549594702116\n",
      "Model coefficients :  [44.18793068 98.97389468 58.17121618]\n",
      "Model score :  0.9999993081899219\n"
     ]
    }
   ],
   "source": [
    "# summary of the model\n",
    "print('Model intercept :', linear_model.intercept_)\n",
    "print('Model coefficients : ', linear_model.coef_)\n",
    "print('Model score : ', linear_model.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4f997294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize this model using Pickle’s dump() function\n",
    "with open(\"linear_regression.pkl\", \"wb\") as f:\n",
    "    pickle.dump(linear_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7541af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize it using the load() function\n",
    "with open(\"linear_regression.pkl\", \"rb\") as f:\n",
    "    unpickled_linear_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "33a4c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model intercept : -0.010109549594702116\n",
      "Model coefficients :  [44.18793068 98.97389468 58.17121618]\n",
      "Model score :  0.9999993081899219\n"
     ]
    }
   ],
   "source": [
    "#serialized model is now loaded and saved into the “unpickled_linear_model” variable. \n",
    "# Let’s check this model’s parameters to ensure that it is the same \n",
    "# as the one we initially created:\n",
    "\n",
    "\n",
    "# summary of the model\n",
    "print('Model intercept :', unpickled_linear_model.intercept_)\n",
    "print('Model coefficients : ', unpickled_linear_model.coef_)\n",
    "print('Model score : ', unpickled_linear_model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bbf7e",
   "metadata": {},
   "source": [
    "## Files with one data type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "015d5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename3= 'PlayTennis.csv'\n",
    "data= np.loadtxt(filename3,\n",
    "    delimiter=',', #String used to separate values \n",
    "    skiprows=2, #Skip the first 2 lines \n",
    "    usecols=[0,2], #Read the 1st and 3rd column \n",
    "    dtype=str) #The type of the resulting array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a136beb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['sunny', 'high'],\n",
       "       ['overcast', 'high'],\n",
       "       ['rainy', 'high'],\n",
       "       ['rainy', 'normal'],\n",
       "       ['rainy', 'normal'],\n",
       "       ['overcast', 'normal'],\n",
       "       ['sunny', 'high'],\n",
       "       ['sunny', 'normal'],\n",
       "       ['rainy', 'normal'],\n",
       "       ['sunny', 'normal'],\n",
       "       ['overcast', 'high'],\n",
       "       ['overcast', 'normal'],\n",
       "       ['rainy', 'high']], dtype='<U8')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81992db",
   "metadata": {},
   "source": [
    "## Files with mixed data type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "10361eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename4= 'wine.csv'\n",
    "\n",
    "data1= np.genfromtxt(filename4,\n",
    "    delimiter=',',\n",
    "    names=True, #Look for column header\n",
    "    dtype=None)\n",
    "data1_array = np.recfromcsv(filename4)\n",
    "#The default dtype of the np.recfromcsv() function is None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9b834d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec.array([(1952, 7.495 , 600, 17.1167, 160, 31, 43183.569),\n",
       "           (1953, 8.0393, 690, 16.7333,  80, 30, 43495.03 ),\n",
       "           (1955, 7.6858, 502, 17.15  , 130, 28, 44217.857),\n",
       "           (1957, 6.9845, 420, 16.1333, 110, 26, 45152.252),\n",
       "           (1958, 6.7772, 582, 16.4167, 187, 25, 45653.805),\n",
       "           (1959, 8.0757, 485, 17.4833, 187, 24, 46128.638),\n",
       "           (1960, 6.5188, 763, 16.4167, 290, 23, 46583.995),\n",
       "           (1961, 8.4937, 830, 17.3333,  38, 22, 47128.005),\n",
       "           (1962, 7.388 , 697, 16.3   ,  52, 21, 48088.673),\n",
       "           (1963, 6.7127, 608, 15.7167, 155, 20, 48798.99 ),\n",
       "           (1964, 7.3094, 402, 17.2667,  96, 19, 49356.943),\n",
       "           (1965, 6.2518, 602, 15.3667, 267, 18, 49801.821),\n",
       "           (1966, 7.7443, 819, 16.5333,  86, 17, 50254.966),\n",
       "           (1967, 6.8398, 714, 16.2333, 118, 16, 50650.406),\n",
       "           (1968, 6.2435, 610, 16.2   , 292, 15, 51034.413),\n",
       "           (1969, 6.3459, 575, 16.55  , 244, 14, 51470.276),\n",
       "           (1970, 7.5883, 622, 16.6667,  89, 13, 51918.389),\n",
       "           (1971, 7.1934, 551, 16.7667, 112, 12, 52431.647),\n",
       "           (1972, 6.2049, 536, 14.9833, 158, 11, 52894.183),\n",
       "           (1973, 6.6367, 376, 17.0667, 123, 10, 53332.805),\n",
       "           (1974, 6.2941, 574, 16.3   , 184,  9, 53689.61 ),\n",
       "           (1975, 7.292 , 572, 16.95  , 171,  8, 53955.042),\n",
       "           (1976, 7.1211, 418, 17.65  , 247,  7, 54159.049),\n",
       "           (1977, 6.2587, 821, 15.5833,  87,  6, 54378.362),\n",
       "           (1978, 7.186 , 763, 15.8167,  51,  5, 54602.193)],\n",
       "          dtype=[('year', '<i4'), ('price', '<f8'), ('winterrain', '<i4'), ('agst', '<f8'), ('harvestrain', '<i4'), ('age', '<i4'), ('francepop', '<f8')])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5b2cfa1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(1952, 7.495 , 600, 17.1167, 160, 31, 43183.569),\n",
       "       (1953, 8.0393, 690, 16.7333,  80, 30, 43495.03 ),\n",
       "       (1955, 7.6858, 502, 17.15  , 130, 28, 44217.857),\n",
       "       (1957, 6.9845, 420, 16.1333, 110, 26, 45152.252),\n",
       "       (1958, 6.7772, 582, 16.4167, 187, 25, 45653.805),\n",
       "       (1959, 8.0757, 485, 17.4833, 187, 24, 46128.638),\n",
       "       (1960, 6.5188, 763, 16.4167, 290, 23, 46583.995),\n",
       "       (1961, 8.4937, 830, 17.3333,  38, 22, 47128.005),\n",
       "       (1962, 7.388 , 697, 16.3   ,  52, 21, 48088.673),\n",
       "       (1963, 6.7127, 608, 15.7167, 155, 20, 48798.99 ),\n",
       "       (1964, 7.3094, 402, 17.2667,  96, 19, 49356.943),\n",
       "       (1965, 6.2518, 602, 15.3667, 267, 18, 49801.821),\n",
       "       (1966, 7.7443, 819, 16.5333,  86, 17, 50254.966),\n",
       "       (1967, 6.8398, 714, 16.2333, 118, 16, 50650.406),\n",
       "       (1968, 6.2435, 610, 16.2   , 292, 15, 51034.413),\n",
       "       (1969, 6.3459, 575, 16.55  , 244, 14, 51470.276),\n",
       "       (1970, 7.5883, 622, 16.6667,  89, 13, 51918.389),\n",
       "       (1971, 7.1934, 551, 16.7667, 112, 12, 52431.647),\n",
       "       (1972, 6.2049, 536, 14.9833, 158, 11, 52894.183),\n",
       "       (1973, 6.6367, 376, 17.0667, 123, 10, 53332.805),\n",
       "       (1974, 6.2941, 574, 16.3   , 184,  9, 53689.61 ),\n",
       "       (1975, 7.292 , 572, 16.95  , 171,  8, 53955.042),\n",
       "       (1976, 7.1211, 418, 17.65  , 247,  7, 54159.049),\n",
       "       (1977, 6.2587, 821, 15.5833,  87,  6, 54378.362),\n",
       "       (1978, 7.186 , 763, 15.8167,  51,  5, 54602.193)],\n",
       "      dtype=[('Year', '<i4'), ('Price', '<f8'), ('WinterRain', '<i4'), ('AGST', '<f8'), ('HarvestRain', '<i4'), ('Age', '<i4'), ('FrancePop', '<f8')])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf299b86",
   "metadata": {},
   "source": [
    "## Importing Flat Files with Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "450a35e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename5= 'salary_data.csv'\n",
    "df= pd.read_csv(filename5,\n",
    "         nrows=5, #Number of rows of file to read \n",
    "         header=0, #Row number to use as col names \n",
    "         sep=',', #Delimiter  to use \n",
    "         comment='#', #Character to split comments \n",
    "         na_values=[\"\"]) #String to recognize as NA/NaN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "06c896f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearsExperience   Salary\n",
       "0              1.1  39343.0\n",
       "1              1.3  46205.0\n",
       "2              1.5  37731.0\n",
       "3              2.0  43525.0\n",
       "4              2.2  39891.0"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc0b9fd",
   "metadata": {},
   "source": [
    "# Exploring Your Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a4327f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "75557f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype([('Year', '<i4'), ('Price', '<f8'), ('WinterRain', '<i4'), ('AGST', '<f8'), ('HarvestRain', '<i4'), ('Age', '<i4'), ('FrancePop', '<f8')])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtype #Data type of array elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f5c9624c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape #Array  dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "e25ad1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #Length of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "273713bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "223101eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearsExperience   Salary\n",
       "0              1.1  39343.0\n",
       "1              1.3  46205.0\n",
       "2              1.5  37731.0\n",
       "3              2.0  43525.0\n",
       "4              2.2  39891.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() #Return first DataFrame rows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fab0d57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsExperience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>39343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.3</td>\n",
       "      <td>46205.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.5</td>\n",
       "      <td>37731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>43525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.2</td>\n",
       "      <td>39891.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YearsExperience   Salary\n",
       "0              1.1  39343.0\n",
       "1              1.3  46205.0\n",
       "2              1.5  37731.0\n",
       "3              2.0  43525.0\n",
       "4              2.2  39891.0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail() #Return last DataFrame rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8fff932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=5, step=1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index #Describe index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "478e0903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['YearsExperience', 'Salary'], dtype='object')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns #Describe DataFrame columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8ec97eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 2 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   YearsExperience  5 non-null      float64\n",
      " 1   Salary           5 non-null      float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 208.0 bytes\n"
     ]
    }
   ],
   "source": [
    "df.info() #Info an DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a9808c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = df.values #Convert a DataFrame to an a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ad5fc85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fad86e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
